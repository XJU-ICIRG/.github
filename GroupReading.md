# XJU-ICIRG
> A curated list of XJU-ICIRG.

## Table of Content

* [Papers](#papers)
	* [Speech enhancement](#SpeechEnhancement)
	* [Mandarin quality evaluation](#MandarinQualityEvaluation)
	* [Speech synthesis](#SpeechSynthesis)
	* [Digital simulators](#DigitalSimulators)
	* [Camouflaged object detection](#CamouflagedObjectDetection)
	* [Multi-temporal remote sensing image change detection](#MultitemporalRemoteSensingImageChangeDetection)
	* [Audio-Visual Navigation](#AudioVisualNavigation)
	* [Visual Language Navigation](#VisualLanguageNavigation)
	* [Dual-arm robot operation](#DualArmRobotOperation)
	* [MobileRobotNavigationAndOperation](#MobileRobotNavigationAndOperation)
	* [UAVAlgorithmResearch](#UAVAlgorithmResearch)
* [Datasets](#datasets)
* [Simulators](#simulators)
* [MISC](#misc)

## <a name="contributing"></a> Contributing
When sending PRs, please put the new paper at the correct chronological position as the following format: <br>

```
* **Paper Title** <br>
*Author(s)* <br>
Conference, Year. [[Paper]](link) [[Code]](link) [[Website]](link)
```

## <a name="papers"></a> Papers

### <a name="SpeechEnhancement"></a> Speech Enhancement


### <a name="MandarinQualityEvaluation"></a> Mandarin Quality Evaluation

### <a name="SpeechSynthesis"></a> SpeechSynthesis

* **Conventional and contemporary approaches used in text to speech synthesis: a review** <br>
Navdeep Kaur, Parminder Singh* <br>
Artificial Intelligence Review, 2023. [[Paper]](https://link.springer.com/article/10.1007/s10462-022-10315-0) [[Code]](link) [[Website]](link)
* **Pronunciation Dictionary-Free Multilingual Speech Synthesis Using Learned Phonetic Representations** <br>
Chang Liu, Zhen-Hua Ling, Ling-Hui Chen* <br>
IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10244053) [[Code]](link) [[Website]](link)
* **ProDiff: Progressive fast diffusion model for high-quality text-to-speech** <br>
Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, Yi Ren* <br>
Proceedings of the 30th ACM International Conference on Multimedia, 2022. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3547855) [[Code]](link) [[Website]](link)
* **Towards Zero-Shot Multi-Speaker Multi-Accent Text-to-Speech Synthesis** <br>
*Mingyang Zhang, Xuehao Zhou, Zhizheng Wu, Haizhou Li* <br>
 IEEE Signal Processing Letters, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10173587) [[Code]](link) [[Website]](link)
* **Emotionally Situated Text-to-Speech Synthesis in User-Agent Conversation** <br>
*Yuchen Liu, Haoyu Zhang, Shichao Liu, Xiang Yin, Zejun Ma* <br>
Proceedings of the 31st ACM International Conference on Multimedia., 2023. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3581783.3613823) [[Code]](link) [[Website]](link)
* **Grad-tts: A diffusion probabilistic model for text-to-speech** <br>
*Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov* <br>
Proceedings of the 38th International Conference on Machine Learning, 2021. [[Paper]](https://proceedings.mlr.press/v139/popov21a.html) [[Code]](link) [[Website]](link)
* **LightGrad: Lightweight Diffusion Probabilistic Model for Text-to-Speech** <br>
*Jie Chen, Xingchen Song, Zhendong Peng, Binbin Zhang, Fuping Pan, Zhiyong Wu* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10096710) [[Code]](link) [[Website]](link)
* **DiffVoice: Text-to-Speech with Latent Diffusion** <br>
*Zhijun Liu， Yiwei Guo， Kai Yu* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10095100) [[Code]](link) [[Website]](link)
* **Grad-StyleSpeech: Any-Speaker Adaptive Text-to-Speech Synthesis with Diffusion Models** <br>
*Minki Kang，Dongchan Min，Sung Ju Hwang* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10095515) [[Code]](link) [[Website]](link)
* **Prompttts: Controllable text-to-speech with text descriptions** <br>
*Zhifang Guo，Yichong Leng，Yihan Wu，Sheng Zhao，Xu Tan* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10096285) [[Code]](link) [[Website]](link)
* **Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech** <br>
*Jaehyeon Kim, Jungil Kong, Juhee Son* <br>
International Conference on Machine Learning, 2021. [[Paper]](https://proceedings.mlr.press/v139/kim21f.html) [[Code]](link) [[Website]](link)
* **Qi-tts: Questioning intonation control for emotional speech synthesis** <br>
*Haobin Tang，Xulong Zhang，Jianzong Wang，Ning Cheng，Jing Xiao* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/document/10095623) [[Code]](link) [[Website]](link)
* **SponTTS: modeling and transferring spontaneous style for TTS** <br>
*Hanzhao Li，Xinfa Zhu，Liumeng Xue，Yang Song，Yunlin Chen，Lei Xie* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10445828) [[Code]](link) [[Website]](link)
* **Naturalspeech: End-to-end text-to-speech synthesis with human-level quality** <br>
*Xu Tan，Jiawei Chen，Haohe Liu，Jian Cong，Chen Zhang，Yanqing Liu，Xi Wang，Yichong Leng，Yuanhao Yi，Lei He，Sheng Zhao* <br>
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10409539) [[Code]](link) [[Website]](link)
* **NSV-TTS: Non-Speech Vocalization Modeling And Transfer In Emotional Text-To-Speech** <br>
*Haitong Zhang，Xinyuan Yu，Yue Lin* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10096033) [[Code]](link) [[Website]](link)
* **PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions** <br>
*Reo Shimizu，Ryuichi Yamamoto，Masaya Kawamura，Yuma Shirahata，Hironori Doi，Tatsuya Komatsu，Kentaro Tachibana* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10448173) [[Code]](link) [[Website]](link)
* **Prosody-Aware Speecht5 for Expressive Neural TTS** <br>
*Yan Deng，Long Zhou，Yuanhao Yi，Shujie Liu，Lei He* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10096530) [[Code]](link) [[Website]](link)
* **Mels-Tts : Multi-Emotion Multi-Lingual Multi-Speaker Text-To-Speech System Via Disentangled Style Tokens** <br>
*Heejin Choi，Jae-Sung Bae，Joun Yeop Lee，Seongkyu Mun，Jihwan Lee，Hoon-Young Cho，Chanwoo Kim* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10446852) [[Code]](link) [[Website]](link)
* **Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis** <br>
*Yuxuan Wang，Daisy Stanton，Yu Zhang，RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, Rif A* <br>
International conference on machine learning, 2018. [[Paper]](https://proceedings.mlr.press/v80/wang18h.html?ref=https://githubhelp.com) [[Code]](link) [[Website]](link)
* **VF-Taco2: Towards Fast and Lightweight Synthesis for Autoregressive Models with Variation Autoencoder and Feature Distillation** <br>
*Yuhao Liu，Cheng Gong，Longbiao Wang，Xixin Wu，Qiuyu Liu，Jianwu Dang* <br>
ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10095640) [[Code]](link) [[Website]](link)
* **EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis** <br>
*Haobin Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao* <br>
INTERSPEECH, 2023. [[Paper]](https://www.isca-archive.org/interspeech_2023/tang23_interspeech.html) [[Code]](link) [[Website]](link)
* **StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness Annotations** <br>
*Sen Liu, Yiwei Guo, Xie Chen, Kai Yu* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10446023) [[Code]](link) [[Website]](link)
* **Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts** <br>
*Shun Lei, Yixuan Zhou, Liyang Chen, Dan Luo, Zhiyong Wu, Xixin Wu, Shiyin Kang, Tao Jiang, Yahui Zhou, Yuxing Han* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10447815) [[Code]](link) [[Website]](link)
* **PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions** <br>
*Guanghou Liu, Yongmao Zhang,Yi Lei,Yunlin Chen,Rui Wang,Zhifei Li,Lei Xie* <br>
INTERSPEECH, 2023. [[Paper]](https://www.isca-archive.org/interspeech_2023/liu23t_interspeech.pdf) [[Code]](link) [[Website]](link)
* **Textrolspeech: A text style control speech corpus with codec language text-to-speech models** <br>
*Shengpeng Ji, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10445879) [[Code]](link) [[Website]](link)
* **Enhancing Multilingual TTS with Voice Conversion Based Data Augmentation and Posterior Embedding** <br>
*Hyun-Wook Yoon，Jin-Seob Kim，Ryuichi Yamamoto，Ryo Terashima，Chan-Ho Song，Jae-Min Kim，Eunwoo Song* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10448471) [[Code]](link) [[Website]](link)
* **VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature** <br>
*Chenpeng Du，Yiwei Guo，Xie Chen，Kai Yu* <br>
INTERSPEECH, 2022. [[Paper]](https://www.isca-archive.org/interspeech_2022/du22b_interspeech.html) [[Code]](link) [[Website]](link)
* **Matcha-TTS: A fast TTS architecture with conditional flow matching** <br>
*Shivam Mehta，Ruibo Tu，Jonas Beskow，Éva Székely，Gustav Eje Henter* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10448291) [[Code]](link) [[Website]](link)
* **Reflow-tts: A rectified flow model for high-fidelity text-to-speech** <br>
*Wenhao Guan，Qi Su，Haodong Zhou，Shiyu Miao，Xingjia Xie，Lin Li，Qingyang Hong* <br>
ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10447822) [[Code]](link) [[Website]](link)



### <a name="DigitalSimulators"></a> DigitalSimulators

[awesome-talking-head-generation-HKUST](https://github.com/harlanhong/awesome-talking-head-generation)
<br>
[awesome-talking-head-generation-SZU](https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis)
<br>
[PaperWithCodes-Talking Head Generation](https://paperswithcode.com/task/talking-head-generation)

* **Human-Computer Interaction System: A Survey of Talking-Head Generation** <br>
Rui Zhen, Wenchao Song, Qiang He, Juan Cao, Lei Shi, and Jia Luo 3* <br>
Electronics, 2023. [[Paper]](https://www.mdpi.com/2079-9292/12/1/218#) [[Code]](link) [[Website]](link)
* **NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis** <br>
Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng* <br>
  ECCV, 2020. [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3503250) [[Code]](link) [[Website]](link)
* **AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis** <br>
Yudong Guo， Keyu Chen，Sen Liang， Yong-Jin Liu， Hujun Bao， Juyong Zhang* <br>
ICCV, 2021. [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.html) [[Code]](link) [[Website]](link)
* **Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation** <br>
Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, Bolei Zhou* <br>
  CCF B, 2022. [[Paper]](https://arxiv.org/pdf/2201.07786.pdf) [[Code]](link) [[Website]](link)
* **Real-time neural radiance talking portrait synthesis via audio-spatial decomposition** <br>
Tang J, Wang K, Zhou H, et al* <br>
  arXiv, 2022. [[Paper]](https://arxiv.org/pdf/2211.12368.pdf) [[Code]](link) [[Website]](link)
* **Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis** <br>
Li J, Zhang J, Bai X, et al.* <br>
   IEEE/CVF, 2023. [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.pdf) [[Code]](link) [[Website]](link)
* **Audio-driven talking-head video generation with diffusion model** <br>
Zhua Y, Zhanga C, Liub Q, et al.* <br>
   IEEE/ICASSP, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10094937) [[Code]](link) [[Website]](link)
* **DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation** <br>
Shen S, Zhao W, Meng Z, et al.* <br>
   IEEE/CVF, 2023. [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf) [[Code]](link) [[Website]](link)
* **Audio-driven Talking Head Generation with Transformer and 3D Morphable Model** <br>
Huang R, Zhong W, Li G.* <br>
   ACM International Conference on Multimedia, 2022. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3503161.3551574) [[Code]](link) [[Website]](link)
* **A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild** <br>
Prajwal K R, Mukhopadhyay R, Namboodiri V P, et al* <br>
   ACM International Conference on Multimedia, 2020. [[Paper]](https://arxiv.org/pdf/2008.10010.pdf?) [[Code]](link) [[Website]](link)
* **NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis** <br>
Bi C, Liu X, Liu Z.* <br>
   arXiv, 2024. [[Paper]](https://arxiv.org/pdf/2401.12568.pdf) [[Code]](link) [[Website]](link)
* **Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis** <br>
Ye Z, Jiang Z, Ren Y, et al.* <br>
   ICLR, 2023. [[Paper]](https://arxiv.org/pdf/2301.13430.pdf) [[Code]](https://github.com/yerfor/GeneFace?tab=readme-ov-file) [[Website]](link)
* **Geneface++: Generalized and stable real-time audio-driven 3d talking face generation** <br>
Ye Z, He J, Jiang Z, et al.* <br>
   arXiv, 2024. [[Paper]](https://arxiv.org/pdf/2301.13430.pdf) [[Code]](https://github.com/yerfor/GeneFacePlusPlus/) [[Website]](link)
* **Facediffuser: Speech-driven 3d facial animation synthesis using diffusion** <br>
Stan S, Haque K I, Yumak Z.* <br>
  ACM SIGGRAPH MIG, 2023. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3623264.3624447) [[Code]](https://github.com/uuembodiedsocialai/FaceDiffuser) [[Website]](link)
* **SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis** <br>
Peng Z, Hu W, Shi Y, et al.* <br>
   CVPR, 2024. [[Paper]](https://arxiv.org/abs/2311.17590) [[Code]](https://github.com/ZiqiaoPeng/SyncTalk) [[Website]](https://ziqiaopeng.github.io/synctalk/)
* **DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields For High-Fidelity Talking Portrait Synthesis** <br>
Su Y, Wang S, Wang H.* <br>
   ICASSP, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10448446) [[Code]](link) [[Website]](link)
* **SD-NeRF: Towards Lifelike Talking Head Animation via Spatially-adaptive Dual-driven NeRFs** <br>
Shen S, Li W, Huang X, et al. * <br>
   IEEE Transactions on Multimedia, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10229247) [[Code]](link) [[Website]](link)
* **Faceformer: Speech-driven 3d facial animation with transformers** <br>
Fan Y, Lin Z, Saito J, et al. * <br>
   CVPR, 2022. [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_FaceFormer_Speech-Driven_3D_Facial_Animation_With_Transformers_CVPR_2022_paper.pdf) [[Code]](https://github.com/EvelynFan/FaceFormer) [[Website]](https://evelynfan.github.io/audio2face/)
* **Dfa-nerf: Personalized talking head generation via disentangled face attributes neural rendering** <br>
Yao S, Zhong R Z, Yan Y, et al. * <br>
  arXiv, 2022. [[Paper]](https://arxiv.org/pdf/2201.00791) [[Code]](link) [[Website]](link)
* **Codetalker: Speech-driven 3d facial animation with discrete motion prior** <br>
Xing J, Xia M, Zhang Y, et al. * <br>
  CVPR, 2023. [[Paper]](http://openaccess.thecvf.com/content/CVPR2023/papers/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.pdf) [[Code]](https://github.com/Doubiiu/CodeTalker) [[Website]](https://doubiiu.github.io/projects/codetalker/)
* **Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation** <br>
Zhang W, Cun X, Wang X, et al. * <br>
  CVPR, 2023. [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.pdf) [[Code]](https://github.com/OpenTalker/SadTalker) [[Website]](https://sadtalker.github.io/)
* **Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis** <br>
Shuai Shen, Wanhua Li, et al. * <br>
  ECCV, 2022. [[Paper]](https://arxiv.org/pdf/2207.11770.pdf) [[Code]](https://github.com/sstzal/DFRF) [[Website]](https://sstzal.github.io/DFRF/)
* **Stereo Radiance Fields (SRF):Learning View Synthesis for Sparse Views of Novel Scenes** <br>
Julian Chibane, Aayush Bansal, et al. * <br>
  CVPR, 2021. [[Paper]](https://virtualhumans.mpi-inf.mpg.de/papers/chibane21SRF/chibane21srf.pdf) [[Code]](https://github.com/jchibane/srf) [[Website]](https://virtualhumans.mpi-inf.mpg.de/srf/)
* **Capture, learning, and synthesis of 3D speaking styles (VOCA)** <br>
Cudeiro D, Bolkart T, Laidlaw C, et al.  * <br>
  CVPR, 2019. [[Paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cudeiro_Capture_Learning_and_Synthesis_of_3D_Speaking_Styles_CVPR_2019_paper.pdf) [[Code]](https://github.com/TimoBolkart/voca) [[Website]](https://voca.is.tue.mpg.de/)
* **Meshtalk: 3d face animation from speech using cross-modality disentanglement** <br>
Richard A, Zollhöfer M, Wen Y, et al. * <br>
  CVPR, 2021. [[Paper]](http://openaccess.thecvf.com/content/ICCV2021/papers/Richard_MeshTalk_3D_Face_Animation_From_Speech_Using_Cross-Modality_Disentanglement_ICCV_2021_paper.pdf) [[Code]](https://github.com/facebookresearch/meshtalk) [[Website]]()
* **Emotalk: Speech-driven emotional disentanglement for 3d face animation** <br>
Peng Z, Wu H, Song Z, et al. * <br>
  CVPR, 2023. [[Paper]](http://openaccess.thecvf.com/content/ICCV2023/papers/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.pdf) [[Code]](https://github.com/psyai-net/EmoTalk_release) [[Website]](https://ziqiaopeng.github.io/emotalk/)
* **Moda: Mapping-once audio-driven portrait animation with dual attentions** <br>
Liu Y, Lin L, Yu F, et al. * <br>
  CVPR, 2023. [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_MODA_Mapping-Once_Audio-driven_Portrait_Animation_with_Dual_Attentions_ICCV_2023_paper.pdf) [[Code]]() [[Website]](https://liuyunfei.net/projects/iccv23-moda/)
* **3d gaussian splatting for real-time radiance field rendering** <br>
Kerbl B, Kopanas G, Leimkühler T, et al. * <br>
  ACM Transactions on Graphics, 2023. [[Paper]](https://inria.hal.science/hal-04088161v2) [[Code]](https://github.com/graphdeco-inria/gaussian-splatting) [[Website]]()
* **GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with Audio-Driven 3D Gaussian Splatting** <br>
Cho K, Lee J, Yoon H, et al. * <br>
  arXiv , 2024. [[Paper]](https://arxiv.org/abs/2404.16012) [[Code]](https://github.com/KU-CVLAB/gaussiantalker) [[Website]](https://ku-cvlab.github.io/GaussianTalker/)
* **4d gaussian splatting for real-time dynamic scene rendering** <br>
Wu G, Yi T, Fang J, et al. * <br>
  arXiv , 2023. [[Paper]](https://arxiv.org/pdf/2310.08528) [[Code]](https://github.com/hustvl/4DGaussians) [[Website]](https://guanjunwu.github.io/4dgs/)
* **TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting** <br>
Li J, Zhang J, Bai X, et al. * <br>
  arXiv , 2024. [[Paper]](https://arxiv.org/pdf/2404.15264) [[Code]](https://github.com/Fictionarry/TalkingGaussian) [[Website]](https://fictionarry.github.io/TalkingGaussian/)
* **CG-NeRF: Conditional Generative Neural Radiance Fields** <br>
Kyungmin Jo*, Gyumin Shim*, et al. * <br>
  AAAI, 2021. [[Paper]](https://arxiv.org/pdf/2112.03517) [[Code]]() [[Website]](https://arxiv.org/pdf/2112.03517)
* **GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis** <br>
Katja Schwarz∗ Yiyi Liao∗, et al. * <br>
  NeurIPS, 2020. [[Paper]](https://proceedings.neurips.cc/paper/2020/file/e92e1b476bb5262d793fd40931e0ed53-Paper.pdf) [[Code]](https://github.com/autonomousvision/graf) [[Website]](https://github.com/autonomousvision/graf)
  


### <a name="CamouflagedObjectDetection"></a> CamouflagedObjectDetection

[github 上的总结的库](https://github.com/visionxiang/awesome-camouflaged-object-detection)

* **基于深度学习的伪装目标检测综述** <br>
史彩娟* <br>
计算机科学与探索, 2022. [[Paper]](https://kns.cnki.net/kcms2/article/abstract?v=-93ivAxQXRq5oFqggHKDnWSybKsWWYymtoYgWGtAgi8nuGXcuG_uLXR_jAGSOkCI5n0UBVIxmQAj4FiKtjWPTVWIsd_moByOY-sG_6zTz1-wVF4Lxzcdtrur9sXK7JC_rFN2HzpMXA8xJln6d_ulHXSR9Aw9e87R&uniplatform=NZKPT&language=CHS) [[Code]](link) [[Website]](link)

* **基于注意力机制和多尺度特征的伪装目标检测** <br>
*蔡俊敏，孙 涵* <br>
计算机技术与发展, 2023. [[Paper]](https://kns.cnki.net/kcms2/article/abstract?v=-93ivAxQXRpT2QPc-xcT3BxvTjoVB26B3X1-Ra6R5qt3g_MrAffEr1D92NFkxjYTqAirYXmTacegFTJi54AuM_mEqRDUh4Vg8JqdxxtdxKMZTZ04VDh2Zkw5ZhPsu0HSvGhcnupM1ufdsOkoUykOnlDWq72DdpB9&uniplatform=NZKPT&language=CHS)[[Code]](link) [[Website]](link)

* **基于通道注意力和边缘融合的伪装目标分割方法** <br>
*詹春兰, 王安志, 王明辉* <br>
计算机应用, 2023. [[Paper]](https://kns.cnki.net/kcms2/article/abstract?v=5DzVwdTmeh9B5fYZyoWQfjxSc1CkQEEnLUuex-zlak4DYGbjTa9XKiNv9gOcIfGm2KJVbvWZkdYlCXbZ4gBYlrfJDEvCVgvFg3F6LVQgQh7uILm_zK9e5OlEMXUj6qbP6Rc69gbL8VS6pTRZS-Kkrg==&uniplatform=NZKPT&language=CHS) [[Code]](link) [[Website]](link)

* **OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers** <br>
*Jialun Pei, Tianyang Cheng, Deng-Ping Fan, He Tang, Chuanbo Chen, Luc Van Gool* <br>
ECCV, 2022. [[Paper]](https://arxiv.org/abs/2207.02255) [[Code]](https://github.com/PJLallen/OSFormer) [[Website]](link)

* **FSNet: Focus Scanning Network for Camouflaged Object Detection** <br>
*Ze Song, Xudong Kang, Xiaohui Wei, Haibo Liu, Renwei Dian, Shutao Li* <br>
IEEE Transactions on Image Processing, 2023. [[Paper]](https://ieeexplore.ieee.org/document/10103836) [[Code]](https://github.com/SongZeHNU/FSNet) [[Website]](link)

* **Feature Shrinkage Pyramid for Camouflaged Object Detection** <br>
*Zhou Huang, Hang Dai, Tian-Zhu Xiang, Shuo Wang, Huai-Xin Chen, Jie Qin, Huan Xiong* <br>
CVPR, 2023. [[Paper]](https://arxiv.org/abs/2303.14816) [[Code]](https://github.com/ZhouHuang23/FSPNet) [[Website]](link)

* **Locate, Refine and Restore: A Progressive Enhancement Network for Camouflaged Object Detection** <br>
*Xiaofei Li, Jiaxin Yang, Shuohao Li, Jun Lei, Jun Zhang, Dong Chen* <br>
IJCAI, 2023. [[Paper]](https://www.ijcai.org/proceedings/2023/124) [[Code]][(link)](https://github.
com/ChunmingHe/FEDER) [[Website]](link)

* **Camouflaged Object Detection with Feature Decomposition and Edge Reconstruction** <br>
*Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li* <br>
CVPR, 2023. [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/html/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.html) [[Code]](https://github.com/ChunmingHe/FEDER) [[Website]](link)

### <a name="MultitemporalRemoteSensingImageChangeDetection"></a> MultitemporalRemoteSensingImageChangeDetection
[github 上的总结的库](https://github.com/wenhwu/awesome-remote-sensing-change-detection)

* **多时相遥感影像的变化检测研究现状与展望** <br>
张祖勋１,姜慧伟２,庞世燕３,胡翔云１,４ * <br>
测绘学报,2022,51(7):1091G1107． [[Paper]](https://www.proquest.com/openview/0f4ddf294a1e677342fe4c899817e233/1?pq-origsite=gscholar&cbl=5229093) [[Code]](link) [[Website]](link)
* **Nearest Neighbor-Based Contrastive Learning for Hyperspectral and LiDAR Data Classification** <br>
Meng Wang , Feng Gao , Member, IEEE, Junyu Dong , Member, IEEE,Heng-Chao Li , Senior Member, IEEE, and Qian Du , Fellow, IEEE * <br>
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 61, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10015054) [[Code]](link) [[Website]](link)
* **Multiview Spatial–Spectral Two-Stream Network for Hyperspectral Image Unmixing** <br>
  Lin Qi, Zhenwei Chen , Feng Gao , Member, IEEE, Junyu Dong , Member, IEEE,
Xinbo Gao , Senior Member, IEEE, and Qian Du , Fellow, IEEE* <br>
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 61, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10018370) [[Code]](link) [[Website]](link)
* **Change Detection From Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network** <br>
*Junjie Wang , Feng Gao , Member, IEEE, Junyu Dong , Member, IEEE, Shan Zhang,and Qian Du , Fellow, IEEE* <br>
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9699382/) [[Code]](link) [[Website]](link)
* **Change Detection From Synthetic Aperture Radar Images via Dual Path Denoising Network** <br>
  Junjie Wang , Feng Gao , Member, IEEE, Junyu Dong , Member, IEEE, Qian Du , Fellow, IEEE, and Heng-Chao Li , Senior Member, IEEE* <br>
  IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9699382) [[Code]](link) [[Website]](link)
* **Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images** <br>
  Lei Ding, Kun Zhu, Daifeng Peng, Hao Tang, Kuiwu Yang and Lorenzo Bruzzone, Fellow, IEEE* <br>
  arXiv:2309.01429v3 [cs.CV] 16 Oct 2023. [[Paper]](https://arxiv.org/abs/2309.01429) [[Code]](link) [[Website]](link)

* **Lightweight Remote Sensing Change Detection With Progressive Feature Aggregation and Supervised Attention** <br>
  Zhenglai Li , Chang Tang , Member, IEEE, Xinwang Liu , Senior Member, IEEE,Wei Zhang , Member, IEEE, Jie Dou, Lizhe Wang , Fellow, IEEE,and Albert Y. Zomaya , 
  Fellow, IEEE <br>
  IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 61, 2023. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10034814) [[Code]](link) [[Website]](link)

* **Change Detection on Remote Sensing Images UsingDual-Branch Multilevel Intertemporal Network** <br>
  Yuchao Feng , Jiawei Jiang , Honghui Xu , and Jianwei Zheng , Member, IEEE* <br>
  IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 61, 2023. [[Paper]](link) [[Code]](link) [[Website]](link)

### <a name="AudioVisualNavigation"></a> AudioVisualNavigation
[github 上的awesome-embodied-vision](https://github.com/ChanganVR/awesome-embodied-vision)
<br>
[github 上的awesome-audio-visual](https://github.com/krantiparida/awesome-audio-visual)
<br>
* **Egocentric Audio-Visual Object Localization** <br>
*Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu* <br>
CVPR, 2023. [[Paper]](https://arxiv.org/pdf/2303.13471.pdf) [[Code]](https://github.com/WikiChao/Ego-AV-Loc) [[Website]](link)

* **Look, Listen, and Act: Towards Audio-Visual Embodied Navigation** <br>
*Chuang Gan Yiwei Zhang Jiajun Wu3 Boqing Gong Joshua B. Tenenbaum <br>
ICRA, 2020. [[Paper]](https://ieeexplore.ieee.org/abstract/document/9197008/) [[Code]](link) [[Website]](link)

* **Curriculum Audiovisual Learning** <br>
*Hu, D, Wang, Z, Xiong, H, Wang, D, Nie, F, Dou, D* <br>
CVPR, 2020. [[Paper]](https://arxiv.org/pdf/2001.09414.pdf) [[Code]](link) [[Website]](link)

* **LEARNING TO SET WAYPOINTS FOR AUDIO-VISUAL NAVIGATION** <br>
*Changan Chen Sagnik Majumder Ziad Al-Halah Ruohan Gao Santhosh K. Ramakrishnan Kristen Grauman <br>
ICLR，2021. [[Paper]](https://arxiv.org/pdf/2008.09622.pdf) [[Code]](link) [[Website]](link)

* **Co-Separating Sounds of Visual Objects** <br>
*Ruohan Gao，Kristen Grauman* <br>
CVPR, 2019. [[Paper]](https://readpaper.com/pdf-annotate/note?pdfId=4544535137244307457&noteId=2166788697184407296) [[Code]](link) [[Website]](link)

* **VISUALVOICE: Audio-Visual Speech Separation with Cross-Modal Consistency** <br>
*Ruohan Gao，Kristen Grauman* <br>
CVPR, 2021. [[Paper]](https://arxiv.org/pdf/2101.03149.pdf) [[Code]](https://github.com/facebookresearch/VisualVoice) [[Website]](link)

### <a name="VisualLanguageNavigation"></a> VisualLanguageNavigation

### <a name="DualArmRobotOperation"></a> DualArmRobotOperation

### <a name="MobileRobotNavigationAndOperation"></a> MobileRobotNavigationAndOperation

### <a name="UAVAlgorithmResearch"></a> UAVAlgorithmResearch

## <a name="datasets"></a> Datasets



## <a name="simulators"></a> Simulators


## <a name="misc"></a> MISC
